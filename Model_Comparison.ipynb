{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# OCR Model Comparison: Base vs Finetuned\n",
                "\n",
                "This notebook evaluates and compares the performance of the base **PaddleOCR-VL** model against the **Tachiwin-BF16** finetuned version on a subset of the `tachiwin/multilingual_ocr_llm` dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers torch Pillow datasets tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import torch\n",
                "from PIL import Image\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoModelForCausalLM, AutoProcessor\n",
                "from tqdm.auto import tqdm\n",
                "import gc\n",
                "\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Metric Calculations\n",
                "We use Levenshtein distance to calculate Character Error Rate (CER) and Word Error Rate (WER)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def levenshtein_distance(s1, s2):\n",
                "    if len(s1) < len(s2):\n",
                "        return levenshtein_distance(s2, s1)\n",
                "    if len(s2) == 0:\n",
                "        return len(s1)\n",
                "\n",
                "    previous_row = range(len(s2) + 1)\n",
                "    for i, c1 in enumerate(s1):\n",
                "        current_row = [i + 1]\n",
                "        for j, c2 in enumerate(s2):\n",
                "            insertions = previous_row[j + 1] + 1\n",
                "            deletions = current_row[j] + 1\n",
                "            substitutions = previous_row[j] + (c1 != c2)\n",
                "            current_row.append(min(insertions, deletions, substitutions))\n",
                "        previous_row = current_row\n",
                "    return previous_row[-1]\n",
                "\n",
                "def calculate_cer(reference, hypothesis):\n",
                "    if not reference:\n",
                "        return 1.0 if hypothesis else 0.0\n",
                "    distance = levenshtein_distance(reference, hypothesis)\n",
                "    return distance / len(reference)\n",
                "\n",
                "def calculate_wer(reference, hypothesis):\n",
                "    ref_words = reference.split()\n",
                "    hyp_words = hypothesis.split()\n",
                "    if not ref_words:\n",
                "        return 1.0 if hyp_words else 0.0\n",
                "    distance = levenshtein_distance(ref_words, hyp_words)\n",
                "    return distance / len(ref_words)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Dataset Loading\n",
                "Loading the first 2000 samples from the test split of `tachiwin/multilingual_ocr_llm`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading dataset...\")\n",
                "ds = load_dataset(\"tachiwin/multilingual_ocr_llm\", split=\"test\", streaming=True)\n",
                "\n",
                "# Take first 2000 samples\n",
                "subset = []\n",
                "for i, sample in tqdm(enumerate(ds), total=2000, desc=\"Fetching samples\"):\n",
                "    if i >= 2000:\n",
                "        break\n",
                "    subset.append(sample)\n",
                "\n",
                "print(f\"Loaded {len(subset)} samples.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Inference Logic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_inference(model, processor, image):\n",
                "    messages = [\n",
                "        {\"role\": \"user\",         \n",
                "         \"content\": [\n",
                "                {\"type\": \"image\", \"image\": image.convert(\"RGB\")},\n",
                "                {\"type\": \"text\", \"text\": \"OCR:\"},\n",
                "            ]\n",
                "        }\n",
                "    ]\n",
                "    inputs = processor.apply_chat_template(\n",
                "        messages, \n",
                "        tokenize=True, \n",
                "        add_generation_prompt=True, \t\n",
                "        return_dict=True,\n",
                "        return_tensors=\"pt\"\n",
                "    ).to(DEVICE)\n",
                "\n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(**inputs, max_new_tokens=1024)\n",
                "    \n",
                "    decoded = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
                "    return decoded"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Base Model Inference\n",
                "Model: `PaddlePaddle/PaddleOCR-VL`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_id_base = \"PaddlePaddle/PaddleOCR-VL\"\n",
                "print(f\"Loading base model: {model_id_base}\")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id_base, trust_remote_code=True, torch_dtype=torch.bfloat16\n",
                ").to(DEVICE).eval()\n",
                "processor = AutoProcessor.from_pretrained(model_id_base, trust_remote_code=True)\n",
                "\n",
                "base_results = []\n",
                "for i, sample in tqdm(enumerate(subset), total=len(subset), desc=\"Base Model Inference\"):\n",
                "    image = sample[\"image\"]\n",
                "    output = run_inference(model, processor, image)\n",
                "    base_results.append(output)\n",
                "    if i % 100 == 0:\n",
                "        print(f\"Sample {i} | Output: {output[:100]}...\")\n",
                "\n",
                "# Cleanup to save memory\n",
                "del model\n",
                "del processor\n",
                "gc.collect()\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Finetuned Model Inference\n",
                "Model: `tachiwin/PaddleOCR-VL-Tachiwin-BF16`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_id_ft = \"tachiwin/PaddleOCR-VL-Tachiwin-BF16\"\n",
                "print(f\"Loading finetuned model: {model_id_ft}\")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id_ft, trust_remote_code=True, torch_dtype=torch.bfloat16\n",
                ").to(DEVICE).eval()\n",
                "processor = AutoProcessor.from_pretrained(model_id_ft, trust_remote_code=True)\n",
                "\n",
                "ft_results = []\n",
                "for i, sample in tqdm(enumerate(subset), total=len(subset), desc=\"Finetuned Model Inference\"):\n",
                "    image = sample[\"image\"]\n",
                "    output = run_inference(model, processor, image)\n",
                "    ft_results.append(output)\n",
                "    if i % 100 == 0:\n",
                "        print(f\"Sample {i} | Output: {output[:100]}...\")\n",
                "\n",
                "# Cleanup\n",
                "del model\n",
                "del processor\n",
                "gc.collect()\n",
                "if torch.cuda.is_available():\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Evaluation and Results Comparison\n",
                "\n",
                "We compare the results and print metrics similar to `ocr_evaluator.py`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "comparison_data = []\n",
                "total_raw_cer = 0\n",
                "total_ft_cer = 0\n",
                "total_raw_wer = 0\n",
                "total_ft_wer = 0\n",
                "count = len(subset)\n",
                "\n",
                "print(f\"{'ID':<15} | {'Raw CER':<9} | {'FT CER':<9} | {'Raw WER':<9} | {'FT WER':<9} | {'Improvement'}\")\n",
                "print(\"-\" * 85)\n",
                "\n",
                "for i, sample in enumerate(subset):\n",
                "    gt = sample[\"text\"]\n",
                "    raw = base_results[i]\n",
                "    ft = ft_results[i]\n",
                "    lang = sample.get(\"language\", \"unk\")\n",
                "    \n",
                "    raw_cer = calculate_cer(gt, raw)\n",
                "    ft_cer = calculate_cer(gt, ft)\n",
                "    raw_wer = calculate_wer(gt, raw)\n",
                "    ft_wer = calculate_wer(gt, ft)\n",
                "    \n",
                "    comparison_data.append({\n",
                "        \"id\": sample[\"id\"],\n",
                "        \"language\": lang,\n",
                "        \"ground_truth\": gt,\n",
                "        \"raw\": raw,\n",
                "        \"finetuned\": ft\n",
                "    })\n",
                "    \n",
                "    total_raw_cer += raw_cer\n",
                "    total_ft_cer += ft_cer\n",
                "    total_raw_wer += raw_wer\n",
                "    total_ft_wer += ft_wer\n",
                "    \n",
                "    improvement = raw_cer - ft_cer\n",
                "    # Print sampled results to avoid cluttering the notebook\n",
                "    if count <= 50 or i % (count // 20) == 0 or i == count - 1:\n",
                "        print(f\"{sample['id']:<15} | {raw_cer:>8.2%} | {ft_cer:>8.2%} | {raw_wer:>8.2%} | {ft_wer:>8.2%} | {improvement:>+10.2%}\")\n",
                "\n",
                "avg_raw_cer = total_raw_cer / count\n",
                "avg_ft_cer = total_ft_cer / count\n",
                "avg_raw_wer = total_raw_wer / count\n",
                "avg_ft_wer = total_ft_wer / count\n",
                "\n",
                "print(\"-\" * 85)\n",
                "print(f\"{'AVERAGE':<15} | {avg_raw_cer:>8.2%} | {avg_ft_cer:>8.2%} | {avg_raw_wer:>8.2%} | {avg_ft_wer:>8.2%} | {avg_raw_cer - avg_ft_cer:>+10.2%}\")\n",
                "\n",
                "print(\"\\n--- Summary ---\")\n",
                "print(f\"Overall Raw Accuracy (1-CER): {1 - avg_raw_cer:.2%}\")\n",
                "print(f\"Overall Finetuned Accuracy (1-CER): {1 - avg_ft_cer:.2%}\")\n",
                "print(f\"Standard Error Reduction: {(avg_raw_cer - avg_ft_cer) / avg_raw_cer if avg_raw_cer > 0 else 0:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Saving and Exporting Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output_file = \"comparison_results.json\"\n",
                "with open(output_file, 'w', encoding='utf-8') as f:\n",
                "    json.dump(comparison_data, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(f\"Results saved to {output_file}\")\n",
                "\n",
                "# Code to download manually if in Colab\n",
                "try:\n",
                "    from google.colab import files\n",
                "    files.download(output_file)\n",
                "except ImportError:\n",
                "    print(\"Manual download required if not in Colab.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
